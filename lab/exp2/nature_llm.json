[
  {
    "journal":"Nature Communications",
    "papers":[
      {
        "title":"DrBioRight\u00a02.0: an LLM-powered bioinformatics chatbot for large-scale cancer functional proteomics analysis",
        "url":"/articles/s41467-025-57430-4",
        "author":[
          "Wei Liu",
          "Jun Li",
          "Yitao Tang",
          "Yining Zhao",
          "Chaozhong Liu",
          "Meiyi Song",
          "Zhenlin Ju",
          "Shwetha V. Kumar",
          "Yiling Lu",
          "Rehan Akbani",
          "Gordon B. Mills",
          "Han Liang"
        ],
        "descirption":"Functional proteomics enhances cancer research by identifying biomarkers and therapeutic targets. Here, the authors develop a cancer functional proteomics resource and introduce a resource using reverse phase protein arrays and antibodies targeting key pathways. To improve accessibility, they introduce DrBioRight, an AI-powered platform for intuitive data analysis and visualization.",
        "volume-and-page-info":"Volume: 16, P: 1-6"
      },
      {
        "title":"Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation",
        "url":"/articles/s41467-025-57426-0",
        "author":[
          "Cheng-Yi Li",
          "Kao-Jung Chang",
          "Cheng-Fu Yang",
          "Hsin-Yu Wu",
          "Wenting Chen",
          "Hritik Bansal",
          "Ling Chen",
          "Yi-Ping Yang",
          "Yu-Chun Chen",
          "Shih-Pin Chen",
          "Shih-Jen Chen",
          "Jiing-Feng Lirng",
          "Kai-Wei Chang",
          "Shih-Hwa Chiou"
        ],
        "descirption":"Multimodal large language models (MLLMs) hold promise for a range of medical applications. Here, the authors use MLLMs for 3D brain CT radiology report generation, demonstrating that combining anatomy-aware model fine-tuning with robust evaluation metrics establishes a comprehensive and effective framework.",
        "volume-and-page-info":"Volume: 16, P: 1-14"
      },
      {
        "title":"Author Correction: LLM-driven multimodal target volume contouring in radiation oncology",
        "url":"/articles/s41467-025-55963-2",
        "author":[
          "Yujin Oh",
          "Sangjoon Park",
          "Hwa Kyung Byun",
          "Yeona Cho",
          "Ik Jae Lee",
          "Jin Sung Kim",
          "Jong Chul Ye"
        ],
        "volume-and-page-info":"Volume: 16, P: 1",
        "description":"no description"
      },
      {
        "title":"LLM-driven multimodal target volume contouring in radiation oncology",
        "url":"/articles/s41467-024-53387-y",
        "author":[
          "Yujin Oh",
          "Sangjoon Park",
          "Hwa Kyung Byun",
          "Yeona Cho",
          "Ik Jae Lee",
          "Jin Sung Kim",
          "Jong Chul Ye"
        ],
        "descirption":"The integration of multimodal knowledge would be essential for radiation oncologist to determine the therapeutic treatment. Here, inspired by the large language models facilitating the integration of textural information and images, this group reports a 3D multimodal clinical target volume delineation model combining image and text-based clinical information for decision-making in radiation oncology.",
        "volume-and-page-info":"Volume: 15, P: 1-14"
      },
      {
        "title":"An automatic end-to-end chemical synthesis development platform powered by large language models",
        "url":"/articles/s41467-024-54457-x",
        "author":[
          "Yixiang Ruan",
          "Chenyin Lu",
          "Ning Xu",
          "Yuchen He",
          "Yixin Chen",
          "Jian Zhang",
          "Jun Xuan",
          "Jianzhang Pan",
          "Qun Fang",
          "Hanyu Gao",
          "Xiaodong Shen",
          "Ning Ye",
          "Qiang Zhang",
          "Yiming Mo"
        ],
        "descirption":"The rise of large language model (LLM) technology offers new opportunities for advancing chemical synthesis. Here, the authors developed an LLM-based reaction development framework (LLM-RDF) to copilot the design and experimental tasks throughout the end-to-end chemical synthesis development.",
        "volume-and-page-info":"Volume: 15, P: 1-16"
      },
      {
        "title":"Incremental accumulation of linguistic context in artificial and biological neural networks",
        "url":"/articles/s41467-025-56162-9",
        "author":[
          "Refael Tikochinski",
          "Ariel Goldstein",
          "Yoav Meiri",
          "Uri Hasson",
          "Roi Reichart"
        ],
        "descirption":"This study reveals how the human brain integrates contextual information differently from Large Language Models. A model that combines short-term and long-term context is introduced, improving predictions of neural activity in higher-order brain regions.",
        "volume-and-page-info":"Volume: 16, P: 1-11"
      },
      {
        "title":"Learning and actioning general principles of cancer cell drug sensitivity",
        "url":"/articles/s41467-025-56827-5",
        "author":[
          "Francesco Carli",
          "Pierluigi Di Chiaro",
          "Mariangela Morelli",
          "Chakit Arora",
          "Luisa Bisceglia",
          "Natalia De Oliveira Rosa",
          "Alice Cortesi",
          "Sara Franceschi",
          "Francesca Lessi",
          "Anna Luisa Di Stefano",
          "Orazio Santo Santonocito",
          "Francesco Pasqualetti",
          "Paolo Aretini",
          "Pasquale Miglionico",
          "Giuseppe R. Diaferia",
          "Fosca Giannotti",
          "Pietro Li\u00f2",
          "Miquel Duran-Frigola",
          "Chiara Maria Mazzanti",
          "Gioacchino Natoli",
          "Francesco Raimondi"
        ],
        "descirption":"Potential anti-tumor therapies remain to be discovered in cancer cell line high-throughput screening datasets. Here, the authors develop a machine learning approach to infer cancer cell drug sensitivity from transcriptomics data and to explore drug mechanisms of action, and predict effective drugs for pancreatic cancer and glioblastoma.",
        "volume-and-page-info":"Volume: 16, P: 1-23"
      },
      {
        "title":"Using large language models to accelerate communication for eye gaze typing users with ALS",
        "url":"/articles/s41467-024-53873-3",
        "author":[
          "Shanqing Cai",
          "Subhashini Venugopalan",
          "Katie Seaver",
          "Xiang Xiao",
          "Katrin Tomanek",
          "Sri Jalasutram",
          "Meredith Ringel Morris",
          "Shaun Kane",
          "Ajit Narayanan",
          "Robert L. MacDonald",
          "Emily Kornman",
          "Daniel Vance",
          "Blair Casey",
          "Steve M. Gleason",
          "Philip Q. Nelson",
          "Michael P. Brenner"
        ],
        "descirption":"Individuals with severe motor impairments use gaze to type and communicate. This paper presents a large language model-based user interface that enables gaze typing in highly abbreviated forms, achieving significant motor saving and speed gain.",
        "volume-and-page-info":"Volume: 15, P: 1-18"
      }
    ]
  },
  {
    "journal":"Nature",
    "papers":[
      {
        "title":"AI hallucinations are a feature of LLM design, not a bug",
        "url":"/articles/d41586-025-00662-7",
        "author":[
          "\n                Joseph Dumit",
          "\n                Andreas Roepstorff"
        ],
        "volume-and-page-info":"Volume: 639, P: 38",
        "description":"no description"
      },
      {
        "title":"\u2018Fighting fire with fire\u2019 \u2014 using LLMs to combat LLM hallucinations",
        "url":"/articles/d41586-024-01641-0",
        "author":[
          "\n                Karin Verspoor"
        ],
        "descirption":"The number of errors produced by an LLM can be reduced by grouping its outputs into semantically similar clusters. Remarkably, this task can be performed by a second LLM, and the method\u2019s efficacy can be evaluated by a third.",
        "volume-and-page-info":"Volume: 630, P: 569-570"
      },
      {
        "title":"Scalable watermarking for identifying large language model outputs",
        "url":"/articles/s41586-024-08025-4",
        "author":[
          "Sumanth Dathathri",
          "Abigail See",
          "Sumedh Ghaisas",
          "Po-Sen Huang",
          "Rob McAdam",
          "Johannes Welbl",
          "Vandana Bachani",
          "Alex Kaskasoli",
          "Robert Stanforth",
          "Tatiana Matejovicova",
          "Jamie Hayes",
          "Nidhi Vyas",
          "Majd Al Merey",
          "Jonah Brown-Cohen",
          "Rudy Bunel",
          "Borja Balle",
          "Taylan Cemgil",
          "Zahra Ahmed",
          "Kitty Stacpoole",
          "Ilia Shumailov",
          "Ciprian Baetu",
          "Sven Gowal",
          "Demis Hassabis",
          "Pushmeet Kohli"
        ],
        "descirption":"A scheme for watermarking the text generated by large language models shows high text quality preservation and detection accuracy and low latency, and is feasible in large-scale-production settings.",
        "volume-and-page-info":"Volume: 634, P: 818-823"
      }
    ]
  },
  {
    "journal":"Scientific Reports",
    "papers":[
      {
        "title":"A framework for mitigating malicious RLHF feedback in LLM training using consensus based reward",
        "url":"/articles/s41598-025-92889-7",
        "author":[
          "Zafaryab Haider",
          "Md Hafizur Rahman",
          "Vijay Devabhaktuni",
          "Shane Moeykens",
          "Prabuddha Chakraborty"
        ],
        "volume-and-page-info":"Volume: 15, P: 1-19",
        "description":"no description"
      },
      {
        "title":"The impact of LLM chatbots on learning outcomes in advanced driver assistance systems education",
        "url":"/articles/s41598-025-91330-3",
        "author":[
          "Mohsin Murtaza",
          "Chi-Tsun Cheng",
          "Bader M. Albahlal",
          "Muhana Magboul Ali Muslam",
          "Mansoor Syed Raza"
        ],
        "volume-and-page-info":"Volume: 15, P: 1-12",
        "description":"no description"
      },
      {
        "title":"Development and validation of a novel AI framework using NLP with LLM integration for relevant clinical data extraction through automated chart review",
        "url":"/articles/s41598-024-77535-y",
        "author":[
          "Mert Marcel Dagli",
          "Yohannes Ghenbot",
          "Hasan S. Ahmad",
          "Daksh Chauhan",
          "Ryan Turlip",
          "Patrick Wang",
          "William C. Welch",
          "Ali K. Ozturk",
          "Jang W Yoon"
        ],
        "volume-and-page-info":"Volume: 14, P: 1-7",
        "description":"no description"
      },
      {
        "title":"Mitigating spatial hallucination in large language models for path planning via prompt engineering",
        "url":"/articles/s41598-025-93601-5",
        "author":[
          "Hongjie Zhang",
          "Hourui Deng",
          "Jie Ou",
          "Chaosheng Feng"
        ],
        "volume-and-page-info":"Volume: 15, P: 1-13",
        "description":"no description"
      },
      {
        "title":"LLM-Twin: mini-giant model-driven beyond 5G digital twin networking framework with semantic secure communication and computation",
        "url":"/articles/s41598-024-69474-5",
        "author":[
          "Yang Hong",
          "Jun Wu",
          "Rosario Morello"
        ],
        "volume-and-page-info":"Volume: 14, P: 1-21",
        "description":"no description"
      },
      {
        "title":"Enhancing intention prediction and interpretability in service robots with LLM and KG",
        "url":"/articles/s41598-024-77916-3",
        "author":[
          "Jincao Zhou",
          "Xuezhong Su",
          "Weiping Fu",
          "Yang Lv",
          "Bo Liu"
        ],
        "volume-and-page-info":"Volume: 14, P: 1-18",
        "description":"no description"
      },
      {
        "title":"ChatGPT performance in assessing musculoskeletal MRI scan appropriateness based on ACR appropriateness criteria",
        "url":"/articles/s41598-025-88925-1",
        "author":[
          "Jin Rong Tan",
          "Daniel Y. Z. Lim",
          "Quan Le",
          "Gita Y. Karande",
          "Lai Peng Chan",
          "Yeong Huei Ng",
          "Daniel S. W. Ting",
          "Sudharsan Madhavan",
          "Hiok Yang Chan",
          "Anh N. T. Tran",
          "Yusheng Keefe Lai"
        ],
        "volume-and-page-info":"Volume: 15, P: 1-12",
        "description":"no description"
      },
      {
        "title":"Enhancement of long-horizon task planning via active and passive modification in large language models",
        "url":"/articles/s41598-025-91448-4",
        "author":[
          "Kazuki Hori",
          "Kanata Suzuki",
          "Tetsuya Ogata"
        ],
        "volume-and-page-info":"Volume: 15, P: 1-21",
        "description":"no description"
      },
      {
        "title":"A large language model for advanced power dispatch",
        "url":"/articles/s41598-025-91940-x",
        "author":[
          "Yuheng Cheng",
          "Huan Zhao",
          "Xiyuan Zhou",
          "Junhua Zhao",
          "Yuji Cao",
          "Chao Yang",
          "Xinlei Cai"
        ],
        "volume-and-page-info":"Volume: 15, P: 1-17",
        "description":"no description"
      },
      {
        "title":"A pilot study of measuring emotional response and perception of LLM-generated questionnaire and human-generated questionnaires",
        "url":"/articles/s41598-024-53255-1",
        "author":[
          "Zhao Zou",
          "Omar Mubin",
          "Fady Alnajjar",
          "Luqman Ali"
        ],
        "volume-and-page-info":"Volume: 14, P: 1-13",
        "description":"no description"
      }
    ]
  },
  {
    "journal":"Nature Medicine",
    "papers":[
      {
        "title":"The TRIPOD-LLM reporting guideline for studies using large language models",
        "url":"/articles/s41591-024-03425-5",
        "author":[
          "Jack Gallifant",
          "Majid Afshar",
          "Saleem Ameen",
          "Yindalon Aphinyanaphongs",
          "Shan Chen",
          "Giovanni Cacciamani",
          "Dina Demner-Fushman",
          "Dmitriy Dligach",
          "Roxana Daneshjou",
          "Chrystinne Fernandes",
          "Lasse Hyldig Hansen",
          "Adam Landman",
          "Lisa Lehmann",
          "Liam G. McCoy",
          "Timothy Miller",
          "Amy Moreno",
          "Nikolaj Munch",
          "David Restrepo",
          "Guergana Savova",
          "Renato Umeton",
          "Judy Wawira Gichoya",
          "Gary S. Collins",
          "Karel G. M. Moons",
          "Leo A. Celi",
          "Danielle S. Bitterman"
        ],
        "descirption":"TRIPOD-LLM (transparent reporting of a multivariable model for individual prognosis or diagnosis\u2013large language model) is a checklist of items considered essential for good reporting of studies that are developing or evaluating an LLM for use in healthcare settings. It is a \u2018living guideline\u2019 that emphasizes transparency, human oversight and task-specific performance reporting.",
        "volume-and-page-info":"Volume: 31, P: 60-69"
      },
      {
        "title":"Integrated image-based deep learning and language models for primary diabetes care",
        "url":"/articles/s41591-024-03139-8",
        "author":[
          "Jiajia Li",
          "Zhouyu Guan",
          "Jing Wang",
          "Carol Y. Cheung",
          "Yingfeng Zheng",
          "Lee-Ling Lim",
          "Cynthia Ciwei Lim",
          "Paisan Ruamviboonsuk",
          "Rajiv Raman",
          "Leonor Corsino",
          "Justin B. Echouffo-Tcheugui",
          "Andrea O. Y. Luk",
          "Li Jia Chen",
          "Xiaodong Sun",
          "Haslina Hamzah",
          "Qiang Wu",
          "Xiangning Wang",
          "Ruhan Liu",
          "Ya Xing Wang",
          "Tingli Chen",
          "Xiao Zhang",
          "Xiaolong Yang",
          "Jun Yin",
          "Jing Wan",
          "Wei Du",
          "Ten Cheer Quek",
          "Jocelyn Hui Lin Goh",
          "Dawei Yang",
          "Xiaoyan Hu",
          "Truong X. Nguyen",
          "Simon K. H. Szeto",
          "Peranut Chotcomwongse",
          "Rachid Malek",
          "Nargiza Normatova",
          "Nilufar Ibragimova",
          "Ramyaa Srinivasan",
          "Pingting Zhong",
          "Wenyong Huang",
          "Chenxin Deng",
          "Lei Ruan",
          "Cuntai Zhang",
          "Chenxi Zhang",
          "Yan Zhou",
          "Chan Wu",
          "Rongping Dai",
          "Sky Wei Chee Koh",
          "Adina Abdullah",
          "Nicholas Ken Yoong Hee",
          "Hong Chang Tan",
          "Zhong Hong Liew",
          "Carolyn Shan-Yeu Tien",
          "Shih Ling Kao",
          "Amanda Yuan Ling Lim",
          "Shao Feng Mok",
          "Lina Sun",
          "Jing Gu",
          "Liang Wu",
          "Tingyao Li",
          "Di Cheng",
          "Zheyuan Wang",
          "Yiming Qin",
          "Ling Dai",
          "Ziyao Meng",
          "Jia Shu",
          "Yuwei Lu",
          "Nan Jiang",
          "Tingting Hu",
          "Shan Huang",
          "Gengyou Huang",
          "Shujie Yu",
          "Dan Liu",
          "Weizhi Ma",
          "Minyi Guo",
          "Xinping Guan",
          "Xiaokang Yang",
          "Covadonga Bascaran",
          "Charles R. Cleland",
          "Yuqian Bao",
          "Elif I. Ekinci",
          "Alicia Jenkins",
          "Juliana C. N. Chan",
          "Yong Mong Bee",
          "Sobha Sivaprasad",
          "Jonathan E. Shaw",
          "Rafael Sim\u00f3",
          "Pearse A. Keane",
          "Ching-Yu Cheng",
          "Gavin Siew Wei Tan",
          "Weiping Jia",
          "Yih-Chung Tham",
          "Huating Li",
          "Bin Sheng",
          "Tien Yin Wong"
        ],
        "descirption":"Tailored to provide diabetes management recommendations from large training and validation datasets, an artificial intelligence system integrating language and computer vision capabilities is shown to improve self-management of patients in a prospective implementation study.",
        "volume-and-page-info":"Volume: 30, P: 2886-2896"
      },
      {
        "title":"GPT-4 assistance for improvement of physician performance on patient care tasks: a randomized controlled trial",
        "url":"/articles/s41591-024-03456-y",
        "author":[
          "Ethan Goh",
          "Robert J. Gallo",
          "Eric Strong",
          "Yingjie Weng",
          "Hannah Kerman",
          "Jason A. Freed",
          "Jos\u00e9phine A. Cool",
          "Zahir Kanjee",
          "Kathleen P. Lane",
          "Andrew S. Parsons",
          "Neera Ahuja",
          "Eric Horvitz",
          "Daniel Yang",
          "Arnold Milstein",
          "Andrew P. J. Olson",
          "Jason Hom",
          "Jonathan H. Chen",
          "Adam Rodman"
        ],
        "descirption":"In a prospective study involving 92 physicians from multiple institutions, access to large language model assistance on top of conventional resources increased a score expressing the quality of their reasoning in addressing patient care while assessing five clinical vignettes.",
        "volume-and-page-info":"P: 1-6"
      },
      {
        "title":"A generalist medical language model for disease diagnosis assistance",
        "url":"/articles/s41591-024-03416-6",
        "author":[
          "Xiaohong Liu",
          "Hao Liu",
          "Guoxing Yang",
          "Zeyu Jiang",
          "Shuguang Cui",
          "Zhaoze Zhang",
          "Huan Wang",
          "Liyuan Tao",
          "Yongchang Sun",
          "Zhu Song",
          "Tianpei Hong",
          "Jin Yang",
          "Tianrun Gao",
          "Jiangjiang Zhang",
          "Xiaohu Li",
          "Jing Zhang",
          "Ye Sang",
          "Zhao Yang",
          "Kanmin Xue",
          "Song Wu",
          "Ping Zhang",
          "Jian Yang",
          "Chunli Song",
          "Guangyu Wang"
        ],
        "descirption":"Trained on a large corpus of medical text and patient records and tested across diseases, with specific focus on rare presentations, an open-source medical language model demonstrates higher accuracy than commercial counterparts across specialties and improves accuracy of clinicians in a reader study.",
        "volume-and-page-info":"P: 1-11"
      },
      {
        "title":"Medical large language models are vulnerable to data-poisoning attacks",
        "url":"/articles/s41591-024-03445-1",
        "author":[
          "Daniel Alexander Alber",
          "Zihao Yang",
          "Anton Alyakin",
          "Eunice Yang",
          "Sumedha Rai",
          "Aly A. Valliani",
          "Jeff Zhang",
          "Gabriel R. Rosenbaum",
          "Ashley K. Amend-Thomas",
          "David B. Kurland",
          "Caroline M. Kremer",
          "Alexander Eremiev",
          "Bruck Negash",
          "Daniel D. Wiggan",
          "Michelle A. Nakatsuka",
          "Karl L. Sangwon",
          "Sean N. Neifert",
          "Hammad A. Khan",
          "Akshay Vinod Save",
          "Adhith Palla",
          "Eric A. Grin",
          "Monika Hedman",
          "Mustafa Nasir-Moin",
          "Xujin Chris Liu",
          "Lavender Yao Jiang",
          "Michal A. Mankowski",
          "Dorry L. Segev",
          "Yindalon Aphinyanaphongs",
          "Howard A. Riina",
          "John G. Golfinos",
          "Daniel A. Orringer",
          "Douglas Kondziolka",
          "Eric Karl Oermann"
        ],
        "descirption":"Large language models can be manipulated to generate misinformation by poisoning of a very small percentage of the data on which they are trained, but a harm mitigation strategy using biomedical knowledge graphs can offer a method for addressing this vulnerability.",
        "volume-and-page-info":"Volume: 31, P: 618-626"
      },
      {
        "title":"An evaluation framework for clinical use of large language models in patient interaction tasks",
        "url":"/articles/s41591-024-03328-5",
        "author":[
          "Shreya Johri",
          "Jaehwan Jeong",
          "Benjamin A. Tran",
          "Daniel I. Schlessinger",
          "Shannon Wongvibulsin",
          "Leandra A. Barnes",
          "Hong-Yu Zhou",
          "Zhuo Ran Cai",
          "Eliezer M. Van Allen",
          "David Kim",
          "Roxana Daneshjou",
          "Pranav Rajpurkar"
        ],
        "descirption":"By simulating realistic doctor\u2013patient conversations, a framework can be applied to large language models to investigate shortcomings and bias in patient interactions, providing insight before actual clinical deployment.",
        "volume-and-page-info":"Volume: 31, P: 77-86"
      },
      {
        "title":"Toward expert-level medical question answering with large language models",
        "url":"/articles/s41591-024-03423-7",
        "author":[
          "Karan Singhal",
          "Tao Tu",
          "Juraj Gottweis",
          "Rory Sayres",
          "Ellery Wulczyn",
          "Mohamed Amin",
          "Le Hou",
          "Kevin Clark",
          "Stephen R. Pfohl",
          "Heather Cole-Lewis",
          "Darlene Neal",
          "Qazi Mamunur Rashid",
          "Mike Schaekermann",
          "Amy Wang",
          "Dev Dash",
          "Jonathan H. Chen",
          "Nigam H. Shah",
          "Sami Lachgar",
          "Philip Andrew Mansfield",
          "Sushant Prakash",
          "Bradley Green",
          "Ewa Dominowska",
          "Blaise Ag\u00fcera y Arcas",
          "Nenad Toma\u0161ev",
          "Yun Liu",
          "Renee Wong",
          "Christopher Semturs",
          "S. Sara Mahdavi",
          "Joelle K. Barral",
          "Dale R. Webster",
          "Greg S. Corrado",
          "Yossi Matias",
          "Shekoofeh Azizi",
          "Alan Karthikesalingam",
          "Vivek Natarajan"
        ],
        "descirption":"With an improved framework for model development and evaluation, a large language model is shown to provide answers to medical questions that are comparable or preferred with respect to those provided by human physicians.",
        "volume-and-page-info":"P: 1-8"
      }
    ]
  },
  {
    "journal":"Nature Machine Intelligence",
    "papers":[
      {
        "title":"What large language models know and what people think they know",
        "url":"/articles/s42256-024-00976-7",
        "author":[
          "Mark Steyvers",
          "Heliodoro Tejeda",
          "Aakriti Kumar",
          "Catarina Belem",
          "Sheer Karny",
          "Xinyue Hu",
          "Lukas W. Mayer",
          "Padhraic Smyth"
        ],
        "descirption":"Understanding how people perceive and interpret uncertainty from large language models (LLMs) is crucial, as users often overestimate LLM accuracy, especially with default explanations. Steyvers et al. show that aligning LLM explanations with their internal confidence improves user perception.",
        "volume-and-page-info":"Volume: 7, P: 221-231"
      },
      {
        "title":"LLM-based agentic systems in medicine and healthcare",
        "url":"/articles/s42256-024-00944-1",
        "author":[
          "Jianing Qiu",
          "Kyle Lam",
          "Guohao Li",
          "Amish Acharya",
          "Tien Yin Wong",
          "Ara Darzi",
          "Wu Yuan",
          "Eric J. Topol"
        ],
        "descirption":"Large language model-based agentic systems can process input information, plan and decide, recall and reflect, interact and collaborate, leverage various tools and act. This opens up a wealth of opportunities within medicine and healthcare, ranging from clinical workflow automation to multi-agent-aided diagnosis.",
        "volume-and-page-info":"Volume: 6, P: 1418-1420"
      },
      {
        "title":"Large language models that replace human participants can harmfully misportray and flatten identity groups",
        "url":"/articles/s42256-025-00986-z",
        "author":[
          "Angelina Wang",
          "Jamie Morgenstern",
          "John P. Dickerson"
        ],
        "descirption":"Large language models are being considered to simulate responses from participants of different backgrounds in computational social science experiments. Here it is shown that this practice can misportray and flatten demographic groups in distinctively harmful ways.",
        "volume-and-page-info":"P: 1-12"
      },
      {
        "title":"What is in your LLM-based framework?",
        "url":"/articles/s42256-024-00896-6",
        "author":[],
        "descirption":"To maintain high standards in clarity and reproducibility, authors need to clearly mention and describe the use of GPT-4 and other large language models in their work.",
        "volume-and-page-info":"Volume: 6, P: 845"
      },
      {
        "title":"Rethinking machine unlearning for large language models",
        "url":"/articles/s42256-025-00985-0",
        "author":[
          "Sijia Liu",
          "Yuanshun Yao",
          "Jinghan Jia",
          "Stephen Casper",
          "Nathalie Baracaldo",
          "Peter Hase",
          "Yuguang Yao",
          "Chris Yuhao Liu",
          "Xiaojun Xu",
          "Hang Li",
          "Kush R. Varshney",
          "Mohit Bansal",
          "Sanmi Koyejo",
          "Yang Liu"
        ],
        "descirption":"Machine unlearning techniques remove undesirable data and associated model capabilities while preserving essential knowledge, so that machine learning models can be updated without costly retraining. Liu et al. review recent advances and opportunities in machine unlearning in LLMs, revisiting methodologies and overlooked principles for future improvements and exploring emerging applications in copyright and privacy safeguards and in reducing sociotechnical harms.",
        "volume-and-page-info":"Volume: 7, P: 181-194"
      },
      {
        "title":"Embodied large language models enable robots to complete complex tasks in unpredictable environments",
        "url":"/articles/s42256-025-01005-x",
        "author":[
          "Ruaridh Mon-Williams",
          "Gen Li",
          "Ran Long",
          "Wenqian Du",
          "Christopher G. Lucas"
        ],
        "descirption":"To function in the real world, autonomous robots will have to respond to unanticipated situations. A vision-language-model-based approach is proposed to solve long-horizon robotic tasks, which can adapt to a dynamic environment.",
        "volume-and-page-info":"P: 1-10"
      },
      {
        "title":"Evolutionary optimization of model merging recipes",
        "url":"/articles/s42256-024-00975-8",
        "author":[
          "Takuya Akiba",
          "Makoto Shing",
          "Yujin Tang",
          "Qi Sun",
          "David Ha"
        ],
        "descirption":"Akiba et al. developed an evolutionary approach to automatically merge artificial intelligence models, creating powerful hybrid models without extensive training. The method produces models with enhanced mathematical and visual capabilities that outperform larger models.",
        "volume-and-page-info":"Volume: 7, P: 195-204"
      },
      {
        "title":"Contextual feature extraction hierarchies converge in large language models and the brain",
        "url":"/articles/s42256-024-00925-4",
        "author":[
          "Gavin Mischler",
          "Yinghao Aaron Li",
          "Stephan Bickel",
          "Ashesh D. Mehta",
          "Nima Mesgarani"
        ],
        "descirption":"Why brain-like feature extraction emerges in large language models (LLMs) remains elusive. Mischler, Li and colleagues demonstrate that high-performing LLMs not only predict neural responses more accurately than other LLMs but also align more closely with the hierarchical language processing pathway in the brain, revealing parallels between these models and human cognitive mechanisms.",
        "volume-and-page-info":"Volume: 6, P: 1467-1477"
      },
      {
        "title":"Bridging the gap between machine confidence and human perceptions",
        "url":"/articles/s42256-025-01013-x",
        "author":[
          "Ming Yin"
        ],
        "descirption":"Users often overestimate the accuracy of large language models (LLMs). A new approach examines user perceptions and finds that aligning LLM explanations with the models\u2019 internal confidence improves user perception.",
        "volume-and-page-info":"P: 1-2"
      }
    ]
  },
  {
    "journal":"npj Science of Learning",
    "papers":[
      {
        "title":"Exploring the potential of LLM to enhance teaching plans through teaching simulation",
        "url":"/articles/s41539-025-00300-x",
        "author":[
          "Bihao Hu",
          "Jiayi Zhu",
          "Yiying Pei",
          "Xiaoqing Gu"
        ],
        "volume-and-page-info":"Volume: 10, P: 1-12",
        "description":"no description"
      }
    ]
  },
  {
    "journal":"Humanities and Social Sciences Communications",
    "papers":[
      {
        "title":"LLM-based collaborative programming: impact on students\u2019 computational thinking and self-efficacy",
        "url":"/articles/s41599-025-04471-1",
        "author":[
          "Yi-Miao Yan",
          "Chuang-Qi Chen",
          "Yang-Bang Hu",
          "Xin-Dong Ye"
        ],
        "volume-and-page-info":"Volume: 12, P: 1-12",
        "description":"no description"
      }
    ]
  },
  {
    "journal":"npj Digital Medicine",
    "papers":[
      {
        "title":"Simulating A/B testing versus SMART designs for LLM-driven patient engagement to close preventive care gaps",
        "url":"/articles/s41746-024-01330-2",
        "author":[
          "Sanjay Basu",
          "Dean Schillinger",
          "Sadiq Y. Patel",
          "Joseph Rigdon"
        ],
        "volume-and-page-info":"Volume: 7, P: 1-8",
        "description":"no description"
      },
      {
        "title":"An active inference strategy for prompting reliable responses from large language models in medical practice",
        "url":"/articles/s41746-025-01516-2",
        "author":[
          "Roma Shusterman",
          "Allison C. Waters",
          "Shannon O\u2019Neill",
          "Marshall Bangs",
          "Phan Luu",
          "Don M. Tucker"
        ],
        "volume-and-page-info":"Volume: 8, P: 1-10",
        "description":"no description"
      },
      {
        "title":"Large language model agents can use tools to perform clinical calculations",
        "url":"/articles/s41746-025-01475-8",
        "author":[
          "Alex J. Goodell",
          "Simon N. Chu",
          "Dara Rouholiman",
          "Larry F. Chu"
        ],
        "volume-and-page-info":"Volume: 8, P: 1-13",
        "description":"no description"
      },
      {
        "title":"Preliminary analysis of the impact of lab results on large language model generated differential diagnoses",
        "url":"/articles/s41746-025-01556-8",
        "author":[
          "Balu Bhasuran",
          "Qiao Jin",
          "Yuzhang Xie",
          "Carl Yang",
          "Karim Hanna",
          "Jennifer Costa",
          "Cindy Shavor",
          "Wenshan Han",
          "Zhiyong Lu",
          "Zhe He"
        ],
        "volume-and-page-info":"Volume: 8, P: 1-15",
        "description":"no description"
      },
      {
        "title":"Red teaming ChatGPT in medicine to yield real-world insights on model behavior",
        "url":"/articles/s41746-025-01542-0",
        "author":[
          "Crystal T. Chang",
          "Hodan Farah",
          "Haiwen Gui",
          "Shawheen Justin Rezaei",
          "Charbel Bou-Khalil",
          "Ye-Jean Park",
          "Akshay Swaminathan",
          "Jesutofunmi A. Omiye",
          "Akaash Kolluri",
          "Akash Chaurasia",
          "Alejandro Lozano",
          "Alice Heiman",
          "Allison Sihan Jia",
          "Amit Kaushal",
          "Angela Jia",
          "Angelica Iacovelli",
          "Archer Yang",
          "Arghavan Salles",
          "Arpita Singhal",
          "Balasubramanian Narasimhan",
          "Benjamin Belai",
          "Benjamin H. Jacobson",
          "Binglan Li",
          "Celeste H. Poe",
          "Chandan Sanghera",
          "Chenming Zheng",
          "Conor Messer",
          "Damien Varid Kettud",
          "Deven Pandya",
          "Dhamanpreet Kaur",
          "Diana Hla",
          "Diba Dindoust",
          "Dominik Moehrle",
          "Duncan Ross",
          "Ellaine Chou",
          "Eric Lin",
          "Fateme Nateghi Haredasht",
          "Ge Cheng",
          "Irena Gao",
          "Jacob Chang",
          "Jake Silberg",
          "Jason A. Fries",
          "Jiapeng Xu",
          "Joe Jamison",
          "John S. Tamaresis",
          "Jonathan H. Chen",
          "Joshua Lazaro",
          "Juan M. Banda",
          "Julie J. Lee",
          "Karen Ebert Matthys",
          "Kirsten R. Steffner",
          "Lu Tian",
          "Luca Pegolotti",
          "Malathi Srinivasan",
          "Maniragav Manimaran",
          "Matthew Schwede",
          "Minghe Zhang",
          "Minh Nguyen",
          "Mohsen Fathzadeh",
          "Qian Zhao",
          "Rika Bajra",
          "Rohit Khurana",
          "Ruhana Azam",
          "Rush Bartlett",
          "Sang T. Truong",
          "Scott L. Fleming",
          "Shriti Raj",
          "Solveig Behr",
          "Sonia Onyeka",
          "Sri Muppidi",
          "Tarek Bandali",
          "Tiffany Y. Eulalio",
          "Wenyuan Chen",
          "Xuanyu Zhou",
          "Yanan Ding",
          "Ying Cui",
          "Yuqi Tan",
          "Yutong Liu",
          "Nigam Shah",
          "Roxana Daneshjou"
        ],
        "volume-and-page-info":"Volume: 8, P: 1-10",
        "description":"no description"
      }
    ]
  },
  {
    "journal":"Eye",
    "papers":[
      {
        "title":"\u201cComparative analysis of large language models against the NHS 111 online triaging for emergency ophthalmology\u201d",
        "url":"/articles/s41433-025-03605-8",
        "author":[
          "Shaheryar Ahmed Khan",
          "Chrishan Gunasekera"
        ],
        "volume-and-page-info":"P: 1-8",
        "description":"no description"
      }
    ]
  },
  {
    "journal":"Nature Methods",
    "papers":[
      {
        "title":"Evaluation of large language models for discovery of gene set function",
        "url":"/articles/s41592-024-02525-x",
        "author":[
          "Mengzhou Hu",
          "Sahar Alkhairy",
          "Ingoo Lee",
          "Rudolf T. Pillich",
          "Dylan Fong",
          "Kevin Smith",
          "Robin Bachelder",
          "Trey Ideker",
          "Dexter Pratt"
        ],
        "descirption":"Large language models show potential in suggesting common functions for a gene set.",
        "volume-and-page-info":"Volume: 22, P: 82-91"
      }
    ]
  },
  {
    "journal":"Nature Reviews Materials",
    "papers":[
      {
        "title":"Large language models for reticular chemistry",
        "url":"/articles/s41578-025-00772-8",
        "author":[
          "Zhiling Zheng\u00a0\n            (\u90d1\u5fd7\u51cc)",
          "Nakul Rampal",
          "Theo Jaffrelot Inizan",
          "Christian Borgs",
          "Jennifer T. Chayes",
          "Omar M. Yaghi"
        ],
        "descirption":"Large language models can accelerate reticular chemistry by guiding material design, data interpretation and discovery. This Perspective discusses how large language models integrate into laboratory workflows and modular artificial intelligence systems, streamlining data-driven research for new materials and tackling crystallization challenges.",
        "volume-and-page-info":"P: 1-13"
      }
    ]
  },
  {
    "journal":"Nature Computational Science",
    "papers":[
      {
        "title":"Harnessing large language models for data-scarce learning of polymer properties",
        "url":"/articles/s43588-025-00768-y",
        "author":[
          "Ning Liu",
          "Siavash Jafarzadeh",
          "Brian Y. Lattimer",
          "Shuna Ni",
          "Jim Lua",
          "Yue Yu"
        ],
        "descirption":"A physics-based training pipeline is developed to help tackle the challenges of data scarcity. The framework aligns large language models to a physically consistent initial state that is fine-tuned for learning polymer properties.",
        "volume-and-page-info":"P: 1-10"
      }
    ]
  },
  {
    "journal":"Communications Biology",
    "papers":[
      {
        "title":"Generative language reconstruction from brain recordings",
        "url":"/articles/s42003-025-07731-7",
        "author":[
          "Ziyi Ye",
          "Qingyao Ai",
          "Yiqun Liu",
          "Maarten de Rijke",
          "Min Zhang",
          "Christina Lioma",
          "Tuukka Ruotsalo"
        ],
        "descirption":"This study presents a method that decode representation from brain recordings as input to a large language model, enabling the generation of language that reflects humans\u2019 perceived semantic content.",
        "volume-and-page-info":"Volume: 8, P: 1-12"
      }
    ]
  },
  {
    "journal":"Nature Human Behaviour",
    "papers":[
      {
        "title":"Large language models surpass human experts in predicting neuroscience results",
        "url":"/articles/s41562-024-02046-9",
        "author":[
          "Xiaoliang Luo",
          "Akilles Rechardt",
          "Guangzhi Sun",
          "Kevin K. Nejad",
          "Felipe Y\u00e1\u00f1ez",
          "Bati Yilmaz",
          "Kangjoo Lee",
          "Alexandra O. Cohen",
          "Valentina Borghesani",
          "Anton Pashkov",
          "Daniele Marinazzo",
          "Jonathan Nicholas",
          "Alessandro Salatiello",
          "Ilia Sucholutsky",
          "Pasquale Minervini",
          "Sepehr Razavi",
          "Roberta Rocca",
          "Elkhan Yusifov",
          "Tereza Okalova",
          "Nianlong Gu",
          "Martin Ferianc",
          "Mikail Khona",
          "Kaustubh R. Patil",
          "Pui-Shee Lee",
          "Rui Mata",
          "Nicholas E. Myers",
          "Jennifer K. Bizley",
          "Sebastian Musslick",
          "Isil Poyraz Bilgin",
          "Guiomar Niso",
          "Justin M. Ales",
          "Michael Gaebler",
          "N. Apurva Ratan Murty",
          "Leyla Loued-Khenissi",
          "Anna Behler",
          "Chloe M. Hall",
          "Jessica Dafflon",
          "Sherry Dongqi Bao",
          "Bradley C. Love"
        ],
        "descirption":"Large language models (LLMs) can synthesize vast amounts of information. Luo et al. show that LLMs\u2014especially BrainGPT, an LLM the authors tuned on the neuroscience literature\u2014outperform experts in predicting neuroscience results and could assist scientists in making future discoveries.",
        "volume-and-page-info":"Volume: 9, P: 305-315"
      }
    ]
  }
]