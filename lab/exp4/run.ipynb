{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909daf61-0ef1-4a53-b30c-7539b7dbcfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963e6d4-37cc-46cb-a878-febc317d8ac3",
   "metadata": {},
   "source": [
    "### 1.（15%）读取数据集data.csv，进⾏数据预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88646108-837f-40c0-9559-26009cf6041e",
   "metadata": {},
   "source": [
    "#### Q1.（5%）选取问卷中的SC155Q01HA,SC155Q02HA,SC155Q03HA,SC155Q04HA,SC155Q05HA5个离散性特征作为特征集，分别介绍这些特征所代表的含义和各⾃取值范围，注意到这些特征名称本⾝较为冗⻓且不易于理解，请对特征名进行简化修改，并删除存在缺失值的行。\n",
    "含义和取值范围见实验报告。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d801a73e-d831-4299-96e9-d2b0bd6ae029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1    2    3    4    5\n",
      "1      2.0  2.0  2.0  1.0  1.0\n",
      "2      2.0  2.0  2.0  2.0  2.0\n",
      "3      2.0  3.0  1.0  1.0  1.0\n",
      "4      2.0  3.0  2.0  2.0  2.0\n",
      "5      2.0  4.0  2.0  2.0  2.0\n",
      "...    ...  ...  ...  ...  ...\n",
      "21899  2.0  3.0  2.0  2.0  2.0\n",
      "21900  3.0  3.0  2.0  2.0  2.0\n",
      "21901  3.0  3.0  3.0  2.0  2.0\n",
      "21902  4.0  3.0  3.0  3.0  3.0\n",
      "21903  3.0  3.0  2.0  2.0  2.0\n",
      "\n",
      "[20681 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data.csv'\n",
    "df = pd.read_csv(file_path, index_col = 0)\n",
    "target = ['SC155Q01HA', 'SC155Q02HA', 'SC155Q03HA', 'SC155Q04HA', 'SC155Q05HA']\n",
    "data = df[target]\n",
    "data_drop = data.dropna()\n",
    "rename = ['1', '2', '3', '4', '5']\n",
    "data_drop.columns = rename\n",
    "print(data_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a5cb8-b84d-47b3-a1b4-880f9c7b90b6",
   "metadata": {},
   "source": [
    "#### Q2.（10%）注意到选取的特征可能存在相同取值（如特征A和B都可能取值0），不便于后续的关联分析过程。请构建项集索引，并依据索引内容进行特征值替换。项集索引字典形式如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046e1ad7-d923-42e6-8381-ef92207f24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1    2     3     4     5\n",
      "1      1.0  5.0   9.0  12.0  16.0\n",
      "2      1.0  5.0   9.0  13.0  17.0\n",
      "3      1.0  6.0   8.0  12.0  16.0\n",
      "4      1.0  6.0   9.0  13.0  17.0\n",
      "5      1.0  7.0   9.0  13.0  17.0\n",
      "...    ...  ...   ...   ...   ...\n",
      "21899  1.0  6.0   9.0  13.0  17.0\n",
      "21900  2.0  6.0   9.0  13.0  17.0\n",
      "21901  2.0  6.0  10.0  13.0  17.0\n",
      "21902  3.0  6.0  10.0  14.0  18.0\n",
      "21903  2.0  6.0   9.0  13.0  17.0\n",
      "\n",
      "[20681 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "ind2val = {}\n",
    "crt_idx = 0\n",
    "\n",
    "for i in rename:\n",
    "    for j in range(1, 5):\n",
    "        ind2val[crt_idx] = f'[{i}]=[{j}]'\n",
    "        crt_idx += 1\n",
    "data_replace = data_drop.copy()\n",
    "\n",
    "i = 0\n",
    "for col in data_drop.columns:\n",
    "    for value in range(1, 5):\n",
    "        data_replace[col]= data_replace[col].replace(value, value+i-1)\n",
    "    i+=4\n",
    "print(data_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ca99a",
   "metadata": {},
   "source": [
    "### 2.（60%）基于预处理后的数据集，编写算法代码进行频繁项集挖掘。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e802a06",
   "metadata": {},
   "source": [
    "#### Q1.（30%）请参考以下 Apriori 产⽣频繁项集的算法流程，⾃⾏编写相应代码，分别以最⼩⽀持度阈值为0.25和0.5，挖掘频繁项集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e7efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'1': {(1.0,): (0.26816885063584933,), (2.0,): (0.3985784052995503,)}, '2': {(6.0,): (0.4188385474590204,), (7.0,): (0.3350901793917122,)}, '3': {(9.0,): (0.3363473719839466,), (10.0,): (0.3700014506068372,)}, '4': {(13.0,): (0.29447318795029254,), (14.0,): (0.4361491223828635,)}, '5': {(17.0,): (0.27334268168850634,), (18.0,): (0.48063439872346597,)}}, 2: {'12': {(2.0, 6.0): (0.2602872201537643,)}, '13': {(2.0, 10.0): (0.2744064600357816,)}, '14': {(2.0, 14.0): (0.26183453411343743,)}, '15': {(2.0, 18.0): (0.27401963154586334,)}, '23': {}, '24': {(6.0, 14.0): (0.2694260432280837,)}, '25': {(6.0, 18.0): (0.2733910352497461,)}, '34': {(10.0, 14.0): (0.27450316715826123,)}, '35': {(10.0, 18.0): (0.2713118321164354,)}, '45': {(14.0, 18.0): (0.33054494463517237,)}}, 3: {'123': {}, '124': {}, '125': {}, '134': {}, '135': {}, '145': {}, '245': {}, '345': {}}}\n",
      "{1: {'1': {}, '2': {}, '3': {}, '4': {}, '5': {}}}\n"
     ]
    }
   ],
   "source": [
    "def Apriori(data, min_support=0.5):\n",
    "    result = {}\n",
    "    col_num = len(data.columns)\n",
    "    val_dict = {}\n",
    "    #找到各列可能取值\n",
    "    for col in data.columns:\n",
    "        val_dict[col] = data[col].unique().tolist()\n",
    "    #计算频繁1项集\n",
    "    r1 = {}\n",
    "    for col in data.columns:\n",
    "        r1[col] = {}\n",
    "        for v in val_dict[col]:\n",
    "            freq = len(data[data[col] == v])/len(data)\n",
    "            if freq >= min_support:\n",
    "                r1[col][(v,)] = (freq,)\n",
    "        result[1] = r1\n",
    "    #计算频繁k项集\n",
    "    for i in range(2, col_num+1):\n",
    "        if (i-1 not in result or  len(result[i-1]) == 1):\n",
    "            break\n",
    "        r = {}\n",
    "        for col1 in result[i-1].keys():\n",
    "            for col2 in result[i-1].keys():\n",
    "                #计算候选k项集\n",
    "                if col1[:-1] == col2[:-1] and col1[-1] < col2[-1]:\n",
    "                    new_col = col1+col2[-1]\n",
    "                    #用于存储可能的取值\n",
    "                    test_val = []\n",
    "                    for v1 in result[i-1][col1].keys():\n",
    "                        for v2 in result[i-1][col2].keys():\n",
    "                            if v1[:-1] == v2[:-1]:\n",
    "                                new_val = (v1)+(v2[-1],)\n",
    "                                test_val.append(new_val)\n",
    "                    if len(test_val) == 0:\n",
    "                        continue\n",
    "                    #计算实际的频率\n",
    "                    actual_val = []\n",
    "                    actual_freq = []\n",
    "                    for v in test_val:\n",
    "                        a = data.copy()\n",
    "                        for j in range(len(new_col)):\n",
    "                            a = a[a[new_col[j]]==v[j]]\n",
    "                        freq = len(a)/len(data)\n",
    "                        if freq >= min_support:\n",
    "                            actual_val.append(v)\n",
    "                            actual_freq.append(freq)\n",
    "                    r[new_col] = {tuple(actual_val[i]): (actual_freq[i],) for i in range(len(actual_val))}\n",
    "        if r:   \n",
    "            result[i] = r\n",
    "    return result\n",
    "print(Apriori(data_replace, 0.25))\n",
    "print(Apriori(data_replace, 0.5))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569fccd",
   "metadata": {},
   "source": [
    "#### Q2.（15%）当最小支持度为0.5时，频繁项集数量较少。请将各特征原始取值为1和2的单元格统一修改其值为0，取值为3和4的单元格统一修改其值为1。重复T1-Q2的项集索引构建过程，并以最小支持度阈值为0.5，挖掘频繁项集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdf7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新前--------------------\n",
      "         1    2    3    4    5\n",
      "1      0.0  0.0  0.0  0.0  0.0\n",
      "2      0.0  0.0  0.0  0.0  0.0\n",
      "3      0.0  1.0  0.0  0.0  0.0\n",
      "4      0.0  1.0  0.0  0.0  0.0\n",
      "5      0.0  1.0  0.0  0.0  0.0\n",
      "...    ...  ...  ...  ...  ...\n",
      "21899  0.0  1.0  0.0  0.0  0.0\n",
      "21900  1.0  1.0  0.0  0.0  0.0\n",
      "21901  1.0  1.0  1.0  0.0  0.0\n",
      "21902  1.0  1.0  1.0  1.0  1.0\n",
      "21903  1.0  1.0  0.0  0.0  0.0\n",
      "\n",
      "[20681 rows x 5 columns]\n",
      "更新后--------------------\n",
      "         1    2    3    4    5\n",
      "1      0.0  0.0  0.0  0.0  0.0\n",
      "2      0.0  0.0  0.0  0.0  0.0\n",
      "3      0.0  3.0  0.0  0.0  0.0\n",
      "4      0.0  3.0  0.0  0.0  0.0\n",
      "5      0.0  3.0  0.0  0.0  0.0\n",
      "...    ...  ...  ...  ...  ...\n",
      "21899  0.0  3.0  0.0  0.0  0.0\n",
      "21900  1.0  3.0  0.0  0.0  0.0\n",
      "21901  1.0  3.0  5.0  0.0  0.0\n",
      "21902  1.0  3.0  5.0  7.0  9.0\n",
      "21903  1.0  3.0  0.0  0.0  0.0\n",
      "\n",
      "[20681 rows x 5 columns]\n",
      "频繁项集--------------------\n",
      "{1: {'1': {(1.0,): (0.625211546830424,)}, '2': {(3.0,): (0.6224553938397563,)}, '3': {(5.0,): (0.547507373918089,)}, '4': {(7.0,): (0.5967312992601905,)}, '5': {(9.0,): (0.6324645810163918,)}}, 2: {'12': {(1.0, 3.0): (0.5143368309075963,)}, '13': {(1.0, 5.0): (0.5080025143851845,)}, '14': {(1.0, 7.0): (0.5007978337604565,)}, '15': {(1.0, 9.0): (0.5123543348967652,)}, '23': {}, '24': {}, '25': {}, '34': {}, '35': {}, '45': {(7.0, 9.0): (0.5176248730719017,)}}, 3: {'123': {}, '124': {}, '125': {}, '134': {}, '135': {}, '145': {}}}\n"
     ]
    }
   ],
   "source": [
    "data_rereplace = data_drop.copy()\n",
    "for col in data_rereplace.columns:\n",
    "    data_rereplace[col]= data_rereplace[col].replace(1, 0)\n",
    "    data_rereplace[col]= data_rereplace[col].replace(2, 0)\n",
    "    data_rereplace[col]= data_rereplace[col].replace(3, 1)\n",
    "    data_rereplace[col]= data_rereplace[col].replace(4, 1)\n",
    "print('更新前'+'-'*20)\n",
    "print(data_rereplace)\n",
    "\n",
    "for i in rename:\n",
    "    for j in range(0, 2):\n",
    "        ind2val[crt_idx] = f'[{i}]=[{j}]'\n",
    "        crt_idx += 1\n",
    "\n",
    "i = 0\n",
    "for col in data_drop.columns:\n",
    "    for value in data_drop[col].unique():\n",
    "        data_rereplace[col]= data_rereplace[col].replace(value, value+i)\n",
    "    i+=2\n",
    "print('更新后'+'-'*20)\n",
    "print(data_rereplace)\n",
    "print('频繁项集'+'-'*20)\n",
    "print(Apriori(data_rereplace, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6c76d",
   "metadata": {},
   "source": [
    "#### Q3.（15%）分析Q1和Q2的结果，你有什么发现？请根据各特征定义，分析产⽣这种情况的原因。\n",
    "见实验报告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12470bec",
   "metadata": {},
   "source": [
    "### 3.（25%）基于T2-Q2得到的频繁项集挖掘结果，编写算法代码进⾏关联规则提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a3062",
   "metadata": {},
   "source": [
    "#### Q1.（15%）请以最⼩置信度阈值为0.8，提取形如X->{1}的关联规则，并输出它们的置信度和提升度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c69c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关联规则: 1.0 -> 3.0,置信度: 0.8226604795050271, 提升度: 1.3216376428682872\n",
      "关联规则: 3.0 -> 1.0,置信度: 0.8263031150469975, 提升度: 1.3216376428682872\n",
      "关联规则: 1.0 -> 5.0,置信度: 0.8125290023201858, 提升度: 1.4840512494024343\n",
      "关联规则: 5.0 -> 1.0,置信度: 0.9278459772145193, 提升度: 1.4840512494024343\n",
      "关联规则: 1.0 -> 7.0,置信度: 0.8010054137664347, 提升度: 1.3423217698811796\n",
      "关联规则: 7.0 -> 1.0,置信度: 0.8392350700915647, 提升度: 1.3423217698811796\n",
      "关联规则: 1.0 -> 9.0,置信度: 0.8194895591647333, 提升度: 1.2957082242420377\n",
      "关联规则: 9.0 -> 1.0,置信度: 0.8100917431192662, 提升度: 1.2957082242420377\n",
      "关联规则: 7.0 -> 9.0,置信度: 0.8674337573940524, 提升度: 1.3715135731396328\n",
      "关联规则: 9.0 -> 7.0,置信度: 0.8184250764525994, 提升度: 1.3715135731396328\n"
     ]
    }
   ],
   "source": [
    "def find_rule(data, confidence=0.8):\n",
    "    d1 = data[1]\n",
    "    d2 = data[2]\n",
    "    for col, val_freq in d2.items():\n",
    "        for val, freq in val_freq.items():\n",
    "            for i in range(len(col)):\n",
    "                target = col[i]\n",
    "                condition = col[:i] + col[i+1:]\n",
    "                target_freq = d1[target][(val[i],)]\n",
    "                condition_freq = d1[condition][(val[(i+1)%2],)]\n",
    "                print(f'关联规则: {val[i]} -> {val[(i+1)%2]},置信度: {freq[0]/target_freq[0]}, 提升度: {freq[0]/(target_freq[0]*condition_freq[0])}')\n",
    "find_rule(Apriori(data_rereplace, 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e868e5",
   "metadata": {},
   "source": [
    "#### Q2.（10%）参考项集索引的对应关系，对以上频繁项集和关联规则结果进⾏简要分析和总结。\n",
    "见实验报告"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
